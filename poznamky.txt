empiricka distribuce - odhad distribuce dat 	
negative log likelyhood = cross entropy 
gradient je smernice tecny
exponentialy weighted average - zapominam stary bety 

Cviceni 
-------
- prejit na tensorflow 2.0 (achjo...)

Prednaska 3 
-----------
vahy = kernel (v kerasu) 
- regularizace -> potreba spis na malejch datech, slozitejch problemech 

L2 regularizace
- nic moc
- citliva na lambdy 

Dropout 
- regularizace 

Inicializace vah 
- random v nejakym intervalu 
- Xavier Glorot -> asi nejlepsi standard 

Cviceni 3
---------
- keras umi aktivacni funkci ve stringu
- binarycrossentropy loss -> 1 sigmoid neuron na outputu 
- sparse -> labely jsou v distribuci 
- nekde neni potreba explicitne dat na posledni vrstvu softmax 
	-> loss na posledni vrstve softmax
	- v kerasu se to tak upee nedela 
	- loss predpoklada, ze dostane distribuci 
 	- ale da se tam dat from_logits=True (coz udela to samy...) ale z modelu pak nepada distribuce
	- vysledek je stejnej 
- example_keras_functional.py 

Regularization 
- dropout
	- after dense layer 
	- not after output layer 
- L2
	- regularizer L1L2 
	- regularizes everything, everywhere
	- necpat tam args.l2 == 0 -> crashne to 
- label smoothing
	- use categorical cross entropy and accuracy 
 	- format of labels in data must be changed 
	- generate whole dist of labels and pass it to model 
		0000010000 instead of 5 -> generate one-hot encoding 

Ensamble 
- template trains models 
- perform ensambling 
- measure individual accuracy on dev data for every model 
- measure accuracy of all sequences of models -> 1, 1-2, 1-3, 1-4, ...
- accuracy of ensamble 
	- manually -> average the outputs 
	- use keras for the ensambling 
		- uses models, average layer
		- can use evaluate on it 

Uppercase 
- NLP task 
- correctly capitalize given text (names, new sentences, cities, etc.)
- one .txt file, one .py file 
- pass baseline 96.5% 
- possible unknown elements 
- alphabet size can be set 
- window -> size of input around the picked character	
	- output 0/1 

Cviceni 4
---------
cartpole
- little training data -> model needs to be regularized
- task is linearly separable -> simple layer is enough
- I could generate more data -> data augmentation 

mnist_cnn
- Conv2D - tri prdele argumentu, relu 
- CB - convolution with batch norm
	run conv layer, no activation, no biases
	run batch normalization (is a layer, I don't need to set any params)
	apply relu 
	(I need 3 layers) 
- maxpool - kernel_size, stride
	maxpool2D layer
- residual connections - hlidat velikosti outputu
	nejde pouzit sequential model -> functional API 
	
cifar_competition
- small dataset
- 32x32 coloured images
- data augmentation - image data generator in keras (has flips)

Prednaska 5
------------
- cutout - jak dropout ale na vstupni vrstve 
	- vyrezu proste ctverec (ale sousedici ctverec)
- dropblock - prej taky celkem dobrej... 
- RoI pooling - divnej pooling
	- najdu region of insterest -> mapuju to na ten hlavni image 
	- regresi se da spocitat kde ten region of interest je 
	- pouziva se Huber loss (neco mezi L1 a L2) 
- intersection over union -> kdyz mam vic regionu pro 2 tridu, spocitam si intersection a doplnek 
	- cim vic jsou podobny tim vetsi bude hodnota (same = 1) 
- non-maximum supression - musim nejak pospojovat ty regiony s vecma, co maji bejt to samy
	- ja dostanu pravdepodobnosti toho ze to je trida i ze tam fakt ta vec je 
	- kdyz se protinam s nekym jistejsim, tak se smazu 

Cviceni 5
---------
- mnist_multiple
	- two digits, first larger than second?
	- keras cannot generate two outputs 
	- run both images through the network, 
		predict both images separately or put them in one vector (v zadani)
- fashion masks 
	- harder than mnist 
	- modified data set -> harder to get the placement 
	- generate mask 
	- trva to docela dlouho...trenoval to asi hodinu 
	- odevzdat python file a text file zase... 

Prednaska 6
-----------
- upscaling convolution -> stride < 1 
	- = transponovana konvoluce 
- LSTM

Cviceni 6
---------
- tf.hub 
	- using trained networks 
- caltech42 competition 
	- images in 42 classes
	- not same size of all images 
	- we can use mobilenet 
	- at least 94% accuracy 
	- cca 45 images per category in train set 
 	- 5 per category in dev set 
	- 10 per test set 
	- mobilenet can be trainable -> marks the weights
- sequence 
	- RNNs 
	- uplne me nezajima zadnej dropout a tak 
	- LSTM cell - faster on GPUs 

Cviceni 7
---------
Task 1
- part-of-speech tagging
- tokenizes sentences (there are lemmas -> ignore)
- original sentence to -> pronoun, verb, noun, . 
- 123k sentences 
- take words -> embed -> bidirectional RNN (v prednasce) 
- concat the results 
- various sized inputs -> problem 
	- batch has multiple sentences -> each different size 
	- padding words (to fill the gaps) 
- each vector -> forms, lemmas, tags 
	- word ids (words mapped to dict) 
	- words (int -> str dict) 
	- words_map (str -> int dict) 
- dev and test sets may contain different words -> <unk> unknown word in the dict 
- also dict has <pad> padding word 
- create densely packed tensor when creating batches 
	- pad it with padding words
- input size -> sentences(longest sentence) x num. words 
- embedding matrix -> for each word -> dimensionality of the embedding 
- embedding computation 
	- embedding layer -> creates matrix with embeddings 
		- size of vocabulary (no_words)
		- dimensions 
=> representation s x w x d
- padding words?
	- mask values in the tensor (in layers and in losses) 
	- set the masking -> specify it on input 
	- masking layer -> specify element to be masked
	- embedding layer -> mask_layer (creates embedding and masks zero index) 
-> dataset uses zero for the <unk> element 
- after running RNN -> s x w x recurrent cell dimension (same output in both ways)
- bidirectional layer -> give it layer and it runs it in both directions 
	- every RNN layer has "go_backwards" 
	- merge_mode (combining outputs) -> concat 
	- run the classification 
	- use maxcrossentropy 
- create batches again 
- train_on_batch again 
 
Task 2
- character level word embeddings 
- charseqs -> all unique words in the batch 
- create alphabet ( + <pad>, <unk> )
- index mapping for each character (charseq_ids) 
- do everything manually 
- embedding -> w x c x d 
- vector od size 2*d for each word in the batch (concat the outputs of bidi runs) 
- tf.gather -> give it ids and matrix -> replacing the corresponding stuff
- same todos -> just copy original solution 
- cannot do train on batch
- mask -> third parameter on the loss 

Task 3 
- convolution 
- again character embeddings 
- words are embedded 
- wxcxd
- use conv -> filters with various kernel sizes
- Conv1D 
- output -> w x c-sth x no_filters
-> global max pooling 
-> concat results for each kernel 
-> add Dense layer to the end -> generates embedding 

Competition task 
- speech recognition (TIMIT dataset) 
- data won't download automaticly -> link to data in recodex
- amplitude 
|window| 
- compute spectrum, move the window (some overlap good)
- script in the repo which does the preprocessing 
-> 26 numbers for each window 
- output 
	- sequence of characters
	- different size than size of sentence on the input -> different loss function
	- lecture 7 -> 46
	- connectionist temporal classification loss 
		- trying to put characters to good places (where the letter happens on the amplitude) 
		- extended labeling 
		- add black symbol (can be generated by network), add duplicates 
	 	- generate normal labeling -> remove neighboring symbols (duplicates) and blanks 
		- we don't have allignment -> CTC solves it 
			- considers all possibilities 
			- just does magic -> but it's already implemented 
			- gets labels (correct letters), logits (the prediction with bordel) 
			- lengths of both 
	- prediction -> genetare logits -> need decoder (already implemented - beam search decoder) 
		- gives most probable decoding (or n most probable decodings) 
- CER 
	- compare gold label and computed one -> compute longest common subsequence 
	- compute edit distance (what to add / get rid of) 
	- edit distance method in tf directly 
- generate annotations for the test set 
- 50% limit 

Cviceni 8
---------
Tagger competition 
- 1,5 mil. words in train dataset 
- 15 character string, 1.5k tags 
- additional data can be used
- word embedding by to mohl dat taky (mozna s vice datasetama)

3D object recognition 
- classify 3d object
- 10 classes
- after creating dataset I will get H,W,D
- can use compile and fit (unless I want to create my own batches)
- can use sequence model
- extension of image classification -> 3D convolution can be used 

Prednaska 9
-----------
seq2seq
- encoder (jedno rnn) 
	- zpracuje vstup
	- na vystupu reprezentace cely sekvence
- druhy rnn 
	- generuje vystupni sekvenci 
	- generuju dokud EOF 
	- potrebuje videt co vygeneroval na vystupu a na zaklade toho generovat dal 
	- ma na vstupu minulej vystup a co cely vygeneroval 
	- prvni bunka dostava START element 
	- encoded reprezentace se da narvat uplne vsude
- ze zacatku to generuje picoviny -> teacher forcing 
	- kdyz vygeneruje blbost -> da se ji spravnej vstup 
	- trenuje se se spravnejma labelama 
- tech RNN bunek se tam proste cpe co to da... 
- fixne velkej vektor na encoded -> blby 
-> ATTENTION
- v kazdym kroku ma decoder jeste vstup kam se chce koukat 
- v encoderu -> mam vystup jeste pro kazdy slovo 
- musi se natrenovat kam chce ten decoder cumet 
- muzu se naucit distribuci toho jak moc se chci nekam koukat 

Cviceni 9
---------
from tensorflow.python import keras 
- bude to doplnovat, ale je to trochu jinej keras, takze to nemusi fungovat

seq2seq 
- lemmatizer
- encoder -> embed each character -> RNN (bidir) -> sum results -> pass to decoder 
- decoder -> input BOW -> every next node gets the previous output on input -> at last character -> output EOW
- how many RNNs? (dynamic number of nodes depending on the decoded output) 
- implement init block -> initialize -> produce first input and first state
- implement step block -> gets inputs and states -> produce output and generate next input and the next state 
- stack the step block as many times as needed 
- both blocks needs information if the sequence ended or not 
- je na to nejakej package -> ten se na to pouzivat nebude 
- same data as in the tagger task 
- store all the layer that will be used
- only cells in decoder (we need more of them)
- we will be processing logits 
- _append_eow appends ends of words
- source states (sipka mezi encoderem a decoderem) 
- po tom co se udela gather -> pouzije se neco co jde pres inputy a kopiruje to jenom non-padding inputy
 	-> dostaneme list individualnich slov co nejsou padding slova 
- output_layer jsou logits 
- batch_size je first dimension of _source_states shapu
- output type je float (logits) 
- embed everything...
- predict batch in evaluation -> run inference 
	- we don't have outputs
- prediction decoder -> doesn't return logits
	- argmax at the output -> feed it as input  

Competition
- same data















 