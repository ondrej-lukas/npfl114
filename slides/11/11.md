title: NPFL114, Lecture 11
class: title, langtech, cc-by-nc-sa

# Speech Synthesis,<br>Reinforcement Learning

## Milan Straka

### May 13, 2019

---
section: WaveNet
# WaveNet

Our goal is to model speech, using a auto-regressive model
$$p(→x) = ∏_t p(x_t | x_{t-1}, …, x_1).$$

~~~
![w=80%,h=center](wavenet_causal_convolutions.pdf)

---
# WaveNet

![w=100%,v=middle](wavenet_dilated_convolutions.pdf)

---
# WaveNet

## Output Distribution
The raw audio is usually stored in 16-bit samples. However, classification
into $65\,536$ classes would not be tractable, and instead WaveNet adopts
$μ$-law transformation and quantize the samples into 256 values using
$$\operatorname{sign}(x)\frac{\ln(1 + 255|x|)}{\ln(1 + 255)}.$$

~~~
## Gated Activation
To allow greater flexibility, the outputs of the dilated convolutions are passed
through the gated activation units
$$→z = \tanh(⇉W_f * →x) ⋅ σ(⇉W_g * →x).$$

---
# WaveNet

![w=80%,h=center](wavenet_block.pdf)

---
# WaveNet

## Global Conditioning
Global conditioning is performed by a single latent representation $→h$,
changing the gated activation function to
$$→z = \tanh(⇉W_f * →x + ⇉V_f→h) ⋅ σ(⇉W_g * →x + ⇉V_g→h).$$

## Local Conditioning
For local conditioning, we are given a timeseries $h_t$, possibly with a lower
sampling frequency. We first use transposed convolutions $→y = f(→h)$ to match resolution
and then compute analogously to global conditioning
$$→z = \tanh(⇉W_f * →x + ⇉V_f * →y) ⋅ σ(⇉W_g * →x + ⇉V_g * →y).$$
---

# WaveNet

The original paper did not mention hyperparameters, but later it was revealed
that:
- 30 layers were used

~~~
  - grouped into 3 dilation stacks with 10 layers each
~~~
  - in a dilation stack, dilation rate increases by a factor of 2, starting
    with rate 1 and reaching maximum dilation of 512
~~~
- filter size of a dilated convolution is 3
~~~
- residual connection has dimension 512
~~~
- gating layer uses 256+256 hidden units
~~~
- the $1×1$ output convolution produces 256 filters
~~~
- trained for $1\,000\,000$ steps using Adam with a fixed learning rate of $0.0002$

---
# WaveNet

![w=85%,h=center](wavenet_results.pdf)

---
section: ParallelWaveNet
# Parallel WaveNet

The output distribution was changed from 256 $μ$-law values to a Mixture of
Logistic (suggested for another paper, but reused in other architectures since):
$$ν ∼ ∑_i π_i \operatorname{logistic}(μ_i, s_i).$$

~~~
The logistic distribution is a distribution with a $σ$ as cumulative density function
(where the mean and steepness is parametrized by $μ$ and $s$). Therefore, we can
write
$$ν ∼ ∑_i π_i \big[σ((x + 0.5 - μ_i) / s_i) - σ((x - 0.5 - μ_i) / s_i)\big].$$
(where we replace -0.5 and 0.5 in the edge cases by $-∞$ and $∞$).

---
# Parallel WaveNet

Auto-regressive (sequential) inference is extremely slow in WaveNet.

~~~
Instead, we use a following trick. We will model $p(x_t)$ as $p(x_t | →z_{≤t})$
for a _random_ $→z$ drawn from a logistic distribution. Then, we compute
$$x_t = z_t ⋅ s(→z_{< t}) + μ(→z_{< t}).$$

~~~
Usually, one iteration of the algorithm does not produce good enough results
– 4 iterations were used by the authors. In further iterations,
$$x^i_t = x^{i-1}_t ⋅ s^i(→x^{i-1}_{< t}) + μ^i(→x^{i-1}_{< t}).$$

---
# Parallel WaveNet

The network is trained using a _probability density distillation_ using
a teacher WaveNet, using KL-divergence as loss.

![w=75%,h=center](parallel_wavenet_distillation.pdf)

---
# Parallel WaveNet

![w=80%,v=middle,h=center](parallel_wavenet_results.pdf)

---
section: Tacotron
# Tacotron

![w=65%,h=center](tacotron.pdf)

---
# Tacotron

![w=65%,h=center,v=middle](tacotron_results.pdf)

---
# Tacotron

![w=100%,v=middle](tacotron_comparison.pdf)

---
section: RL
class: center, middle
# Reinforcement Learning

# Reinforcement Learning

---
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s – Richard Bellman
~~~
- Trial and error learning – since 1850s
  - Law and effect – Edward Thorndike, 1911
  - Shannon, Minsky, Clark&Farley, … – 1950s and 1960s
  - Tsetlin, Holland, Klopf – 1970s
  - Sutton, Barto – since 1980s
~~~
- Arthur Samuel – first implementation of temporal difference methods
  for playing checkers

---
# Notable Successes of Reinforcement Learning

- IBM Watson in Jeopardy – 2011
~~~
- Human-level video game playing (DQN) – 2013 (2015 Nature), Mnih. et al, Deepmind
  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games
~~~
- A3C – 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games
~~~
- Rainbow – 2017
  - human-normalized median: 153%
~~~
- Impala – Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

---
# Notable Successes of Reinforcement Learning
- AlphaGo
  - Mar 2016 – beat 9-dan professional player Lee Sedol
~~~
- AlphaGo Master – Dec 2016
  - beat 60 professionals
  - beat Ke Jie in May 2017
~~~
- AlphaGo Zero – 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero – Dec 2017
  - self-play only
  - defeated AlphaGo Zero after 34 hours of training (21 million games)
  - impressive chess and shogi performance after 9h and 12h, respectively

---
# Notable Successes of Reinforcement Learning

- Dota2 – Aug 2017
  - won 1v1 matches against a professional player
~~~
- MERLIN – Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation
~~~
- FTW – Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - trained solely by self-play on 450k games
    - each 5 minutes, 4500 agent steps (15 per second)
~~~
- OpenAI Five – Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs
    - 180 years of experience per day
~~~
- AlphaStar – Jan 2019
  - played 11 games against StarCraft II professionals,
    reaching 10 wins and 1 loss

---
# Notable Successes of Reinforcement Learning

- Neural Architecture Search – 2017
  - automatically designing CNN image recognition networks
    surpassing state-of-the-art performance
~~~
  - AutoML: automatically discovering 
    - architectures (CNN, RNN, overall topology)
    - activation functions
    - optimizers
    - …
~~~
- System for automatic control of data-center cooling – 2017

---
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.pdf)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, …

~~~
Let $q_*(a)$ be the real _value_ of an action $a$:
$$q_*(a) = 𝔼[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) ≝ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a _greedy action_ $A_t$ as
$$A_t(a) ≝ \argmax_a Q_t(a).$$

---
# $ε$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

~~~

An _$ε$-greedy_ method follows the greedy action with probability $1-ε$, and
chooses a uniformly random action with probability $ε$.

---
# $ε$-greedy Method

![w=52%,h=center,v=middle](e_greedy.pdf)

---
# $ε$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} ∑_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} ∑_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $ε$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.pdf)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.pdf)

~~~~
# Markov Decision Process

![w=55%,h=center](diagram.pdf)

A _Markov decision process_ (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:
- $𝓢$ is a set of states,
~~~
- $𝓐$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a _reward_ $r ∈ ℝ$,
~~~
- $γ ∈ [0, 1]$ is a _discount factor_ (we will always use $γ=1$).

~~~
Let a _return_ $G_t$ be $G_t ≝ ∑_{k=0}^∞ γ^k R_{t + 1 + k}$. The goal is to optimize $𝔼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $𝓢=\{S\}$;
~~~
- an action for every arm, $𝓐=\{a_1, a_2, …, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $𝓝(μ_i, σ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = 𝓝(r | μ_i, σ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to have
separate reward distribution for every state. Such generalization is
usually called _Contextualized Bandits_ problem.
Assuming that state transitions are independent on rewards and given by
a distribution $\textit{next}(s)$, the MDP dynamics function for contextualized
bandits problem is given by
$$p(s', r | s, a_i) = 𝓝(r | μ_{i,s}, σ_{i,s}^2) ⋅ \textit{next}(s'|s).$$

---
# (State-)Value and Action-Value Functions

A _policy_ $π$ computes a distribution of actions in a given state, i.e.,
$π(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define _value function_ $v_π(s)$, or
_state-value function_, as
$$v_π(s) ≝ 𝔼_π\left[G_t \middle| S_t = s\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An _action-value function_ for a policy $π$ is defined analogously as
$$q_π(s, a) ≝ 𝔼_π\left[G_t \middle| S_t = s, A_t = a\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Evidently,
$$\begin{aligned}
  v_π(s) &= 𝔼_π[q_π(s, a)], \\
  q_π(s, a) &= 𝔼_π[R_{t+1} + γv_π(S_{t+1}) | S_t = s, A_t = a].
\end{aligned}$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) ≝ \max_π v_π(s),$$
analogously
$$q_*(s, a) ≝ \max_π q_π(s, a).$$

~~~
Any policy $π_*$ with $v_{π_*} = v_*$ is called an _optimal policy_. Such policy
can be defined as $π_*(s) ≝ \argmax_a q_*(s, a) = \argmax_a 𝔼[R_{t+1} + γv_*(S_{t+1}) | S_t = s, A_t = a]$.

~~~
## Existence
Under some mild assumptions, there always exists a unique optimal state-value function,
unique optimal action-value function, and (not necessarily unique) optimal policy.
The mild assumptions are that either termination is guaranteed from all
reachable states, or $γ < 1$.

---
section: MonteCarlo
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $𝓢$ and we will store
estimates for each of them.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

---
# Monte Carlo Methods

To guarantee convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume _exploring starts_, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. The literature distinguishes two cases:
- _first visit_: only the first occurence of a state-action pair in an episode is
  considered
- _every visit_: all occurences of a state-action pair are considered.

Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.pdf)


---
# Monte Carlo and $ε$-soft Policies

A policy is called $ε$-soft, if
$$π(a|s) ≥ \frac{ε}{|𝓐(s)|}.$$

~~~
For $ε$-soft policy, Monte Carlo policy evaluation also converges, without the need
of exploring starts.

~~~
We call a policy $ε$-greedy, if one action has maximum probability of
$1-ε+\frac{ε}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for the class of $ε$-soft
policies, and using<br>$ε$-greedy policy in policy improvement step, policy
iteration has the same convergence properties. (We can embed the $ε$-soft behaviour
“inside” the environment and prove equivalence.)

---
# Monte Carlo for $ε$-soft Policies

### On-policy every-visit Monte Carlo for $ε$-soft Policies
Algorithm parameter: small $ε>0$

Initialize $Q(s, a) ∈ ℝ$ arbitrarily (usually to 0), for all $s ∈ 𝓢, a ∈ 𝓐$<br>
Initialize $C(s, a) ∈ ℤ$ to 0, for all $s ∈ 𝓢, a ∈ 𝓐$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, …, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $ε$, generate a random uniform action
  - Otherwise, set $A_t ≝ \argmax\nolimits_a Q(S_t, a)$
- $G ← 0$
- For each $t=T-1, T-2, …, 0$:
  - $G ← γG + R_{T+1}$
  - $C(S_t, A_t) ← C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: REINFORCE
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_→θ v_π(s)$.

---
# Policy Gradient Methods

In addition to discarding $ε$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.pdf)

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{h, π} v_π(s)$.

~~~
Then
$$∇_→θ v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ)$$
and
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is probability of transitioning from state $s$ to $s'$
using 0, 1, … steps.



---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ \big(∑\nolimits_{s'} p(s'|s, a)(r + v_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
                \qquad\qquad\qquad ∑\nolimits_{a'} \Big[ ∇ π(a'|s'; →θ) q_π(s', a') + π(a'|s'; →θ) \Big(∑\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\Big) \big) \Big]$

~~~
_Continuing to expand all $v_π(s'')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ).$

---
# Proof of Policy Gradient Theorem

Recall that the initial state distribution is $h(s)$ and the on-policy
distribution under $π$ is $μ(s)$. If we let $η(s)$ denote the number
of time steps spent, on average, in state $s$ in a single episode,
we have
$$η(s) = h(s) + ∑_{s'}η(s') ∑_a π(a|s') p(s|s',a).$$

~~~
The on-policy distribution is then the normalization of $η(s)$:
$$μ(s) ≝ \frac{η(s)}{∑_{s'} η(s')}.$$

~~~
The last part of the policy gradient theorem follows from the fact that $μ(s)$ is
$$μ(s) = 𝔼_{s_0 ∼ h(s)} P(s_0 → … → s | π).$$

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(→θ) ≝ 𝔼_{h, π} v_π(s)$. The loss is defined as
$$\begin{aligned}
  -∇_→θ J(→θ) &∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ) \\
              &= 𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).
\end{aligned}$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$-∇_→θ J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ \ln π(a | s; →θ),$$
where we used the fact that
$$∇_→θ \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_→θ π(a | s; →θ).$$

To compute the gradient
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$-𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ \ln π(a | s; →θ),$$
estimating the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.pdf)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_→θ π(a | s; →θ).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_→θ π(a | s; →θ) = b(s) ∑_a ∇_→θ π(a | s; →θ) = b(s) ∇1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_π(s, a) - v_π(s)$ function is also called an _advantage function_
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

~~~
Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.pdf)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.pdf)
