title: NPFL114, Lecture 12
class: title, langtech, cc-by-nc-sa

# Transformer, External Memory Networks

## Milan Straka

### May 20, 2019

---
section: Organization
# Exams

Five questions, written preparation, then we go through it together (or you can
leave and let me grade it by myself).

Each question is 20 points, and up to 40 points (surplus above 80 points; there
is no distinction between regular and competition points) transfered from the
practicals, and up to 10 points for GitHub pull requests.

To pass the exam, you need to obtain at least 60, 75 and 90 out of 100 points
for the written exam (plus up to 40 points from the practicals), to obtain
grades 3, 2 and 1, respectively.

The SIS should give you an exact time of the exam (including a gap between
students) so that you do not come all at once.

---
# What Next

In the winter semester:

~~~
## NPFL117 – Deep Learning Seminar [0/2 Ex]

Reading group of deep learning papers (in all areas). Every participant presents
a paper about deep learning, learning how to read a paper, present it in
a understandable way, and get deep learning knowledge from other presentations.

~~~
## NPFL122 – Deep Reinforcement Learning [2/2 C+Ex]

In a sense continuation of Deep Learning, but instead of supervised
learning, reinforced learning is the main method. Similar format to the Deep
Learning course.

---
section: NASNet
# Neural Architecture Search (NASNet) – 2017

- Using REINFORCE with baseline, we can design neural network architectures.

~~~
- We fix the overall architecture and design only Normal and Reduction cells.
![w=27%,h=center](nasnet_overall.pdf)

---
# Neural Architecture Search (NASNet) – 2017

- Every block is designed by a RNN controller generating individual operations.

![w=100%](nasnet_rnn_controller.pdf)

---
# Neural Architecture Search (NASNet) – 2017

![w=80%,h=center](nasnet_blocks.pdf)

---
# Neural Architecture Search (NASNet) – 2017

![w=100%,v=middle](nasnet_performance.pdf)

---
# Neural Architecture Search (NASNet) – 2017

![w=100%,v=middle](../01/nas_net.pdf)

---
section: Transformer
# Attention is All You Need

For some sequence processing tasks, _sequential_ processing of its elements
might be too restricting. Instead, we may want to combine sequence elements
independently on their distance.

---
# Attention is All You Need

![w=33%,h=center](transformer.pdf)

---
# Attention is All You Need

The attention module for a queries $⇉Q$, keys $⇉K$ and values $⇉V$ is defined as:

$$\textrm{Attention}(⇉Q, ⇉K, ⇉V) = \softmax\left(\frac{⇉Q ⇉K^\top}{\sqrt{d_k}}\right)⇉V.$$

The queries, keys and values are computed from current word representations $⇉W$
using a linear transformation as
$$\begin{aligned}
  ⇉Q &= ⇉V_Q ⋅ ⇉W \\
  ⇉K &= ⇉V_K ⋅ ⇉W \\
  ⇉V &= ⇉V_V ⋅ ⇉W \\
\end{aligned}$$

---
# Attention is All You Need

Multihead attention is used in practice. Instead of using one huge attention, we
split queries, keys and values to several groups (similar to how ResNeXt works),
compute the attention in each of the groups separately, and then concatenate the
results.

![w=75%,h=center](transformer_multihead.pdf)

---
# Attention is All You Need

## Positional Embeddings

We need to encode positional information (which was implicit in RNNs).

~~~
- Learned embeddings for every position.

~~~
- Sinusoids of different frequencies:
  $$\begin{aligned}
    \textrm{PE}_{(\textit{pos}, 2i)} & = \sin\left(\textit{pos} / 10000^{2i/d}\right) \\
    \textrm{PE}_{(\textit{pos}, 2i + 1)} & = \cos\left(\textit{pos} / 10000^{2i/d}\right)
  \end{aligned}$$

  This choice of functions should allow the model to attend to relative
  positions, since for any fixed $k$, $\textrm{PE}_{\textit{pos} + k}$ is
  a linear function of $\textrm{PE}_\textit{pos}$.

---
# Attention is All You Need

Positional embeddings for 20 words of dimension 512, lighter colors representing
values closer to 1 and darker colors representing values closer to -1.
![w=60%,h=center](transformer_positional_embeddings.png)

---
# Attention is All You Need

## Regularization

The network is regularized by:
- dropout of input embeddings,
~~~
- dropout of each sub-layer, just before before it is added to the residual
  connection (and then normalized),
~~~
- label smoothing.

~~~
Default dropout rate and also label smoothing weight is 0.1.

~~~
## Parallel Execution
Training can be performed in parallel because of the _masked attention_ – the
softmax weights of the self-attention are zeroed out not to allow attending
words later in the sequence.

~~~
However, inference is still sequential (and no substantial improvements have
been achieved on parallel inference similar to WaveNet).

---
# Why Attention

![w=100%,v=middle](transformer_attentions.pdf)

---
# Transformers Results

![w=100%,v=middle](transformer_results.pdf)

---
# Transformers Results

![w=78%,h=center](transformer_ablations.pdf)

---
section: NTM
# Neural Turing Machines

So far, all input information was stored either directly in network weights, or
in a state of a recurrent network.

~~~
However, mammal brains seem to operate with a _working memory_ – a capacity for
short-term storage of information and its rule-based manipulation.

~~~
We can therefore try to introduce an external memory to a neural network. The
memory $⇉M$ will be a matrix, where rows correspond to memory cells.

---
# Neural Turing Machines

The network will control the memory using a controller which reads from the
memory and writes to is. Although the original paper also considered
a feed-forward (non-recurrent) controller, usually the controller is a recurrent
LSTM network.

![w=55%,h=center](ntm_architecture.pdf)

---
# Neural Turing Machine

## Reading

To read the memory in a differentiable way, the controller at time $t$ emits
a read distribution $→w_t$ over memory locations, and the returned read vector $→r_t$
is then
$$→r_t = ∑_i w_t(i) ⋅ →M_t(i).$$

## Writing

Writing is performed in two steps – an _erase_ followed by an _add_. The
controller at time $t$ emits a write distribution $→w_t$ over memory locations,
and also an _erase vector_ $→e_t$ and an _add vector_ $→a_t$. The memory is then
updates as
$$→M_t(i) = →M_{t-1}(i)\big[1 - w_t(i)→e_t] + w_t(i) →a_t.$$

---
# Neural Turing Machine

The addressing mechanism is designed to allow both
- content addressing, and
- location addressing.

![w=90%,h=center](ntm_addressing.pdf)

---
# Neural Turing Machine

## Content Addressing

Content addressing starts by the controller emitting the _key vector_ $→k_t$,
which is compared to all memory locations $→M_t(i)$, generating a distribution
using a $\softmax$ with temperature $β_t$.
$$w_t^c(i) = \frac{\exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(i))}{∑_j \exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(j))}$$

The $\operatorname{distance}$ measure is usually the cosine similarity
$$\operatorname{distance}(→a, →b) = \frac{→a ⋅ →b}{||→a|| ⋅ ||→b||}.$$

---
# Neural Turing Machine

## Location-Based Addressing

To allow iterative access to memory, the controller might decide to reuse the
memory location from previous timestep. Specifically, the controller emits
_interpolation gate_ $g_t$ and defines
$$→w_t^g = g_t →w_t^c + (1 - g_t) →w_{t-1}.$$

Then, the current weighting may be shifted, i.e., the controller might decide to
“rotate” the weights by a small integer. For a given range (the simplest case
are only shifts $\{-1, 0, 1\}$), the network emits $\softmax$ distribution over
the shifts, and the weights are then defined using a circular convolution
$$w̃_t(i) = ∑_j w_t^g(j) s_t(i - j).$$

Finally, not to lose precision over time, the controller emits
a _sharpening factor_ $γ_t$ and the final memory location weights are
$w_t(i) = {w̃_t(i)^{γ_t}} / {∑_j w̃_t(j)^{γ_t}}.$

---
# Neural Turing Machines

## Copy Task

Repeat the same sequence as given on input. Trained with sequences of length up
to 20.

![w=70%,h=center](ntm_copy_training.pdf)

---
# Neural Turing Machines

![w=84%,h=center](ntm_copy_generalization.pdf)

---
# Neural Turing Machines

![w=95%,h=center](ntm_copy_generalization_lstm.pdf)

---
# Neural Turing Machines

![w=65%,h=center](ntm_copy_memory.pdf)

---
# Neural Turing Machines

## Associative Recall

In associative recall, a sequence is given on input, consisting of subsequences
of length 3. Then a randomly chosen subsequence is presented on input and the
goal is to produce the following subsequence.

![w=65%,h=center](ntm_associative_recall_training.pdf)

---
# Neural Turing Machines

![w=83%,h=center](ntm_associative_recall_generalization.pdf)

---
# Neural Turing Machines

![w=53%,h=center](ntm_associative_recall_memory.pdf)

---
section: DNC
# Differentiable Neural Computer

NTM was later extended to a Differentiable Neural Computer.

![w=82%,h=center](dnc_architecture.pdf)

---
# Differentiable Neural Computer

The DNC contains multiple read heads and one write head.

~~~
The controller is a deep LSTM network, with input at time $t$ being the current
input $→x_t$ and $R$ read vectors $→r_{t-1}^1, …, →r_{t-1}^R$ from previous time
step. The output of the controller are vectors $(→ν_t, →ξ_t)$, and the final
output is $→y_t = →ν_t + W_r\big[→r_t^1, …, →r_t^R\big]$.

~~~
In DNC, usage of every memory location is tracked, which allows us to define allocation weighting.
Furthermore, for every memory location we track which memory location
was written to previously ($→b_t$) and subsequently ($→f_t$).

~~~
The, the write weighting is defined as a weighted combination of the allocation
weighting and write content weighting, and read weighting is computed as a weighted
combination of read content weighting, previous write weighting, and subsequent
write weighting.


---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks.pdf)

---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks_traversal.pdf)

---
section: MANN
# Memory-augmented Neural Networks

![w=100%,v=middle](mann_overview.pdf)

---
# Memory-augmented NNs

![w=90%,mw=62%](mann_reading.pdf)![w=38%](mann_writing.pdf)

---
# Memory-augmented NNs

![w=60%,h=center](mann_results.pdf)
